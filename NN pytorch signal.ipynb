{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.special import expit, logit, softmax\n",
    "\n",
    "\n",
    "L = 200\n",
    "sigma = 0.2\n",
    "eta = 0.09\n",
    "\n",
    "input = np.array([[-3],[-1],[1],[3]])\n",
    "output = np.array([[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]])\n",
    "#output = np.array([[-3],[-1],[1],[3]])\n",
    "\n",
    "\n",
    "data_set_t = []\n",
    "data_set_x = []\n",
    "for i in range(L):\n",
    "    n = numpy.random.randint(0,4)\n",
    "    data_set_t.append(output[n])\n",
    "    data_set_x.append(input[n])\n",
    "\n",
    "#data_set_t_noise = data_set_t + sigma*np.random.randn(len(data_set_t),4)\n",
    "\n",
    "data_set = np.hstack((data_set_x,data_set_t))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return expit(-x) * (1-expit(-x))\n",
    "\n",
    "percent_for_train = 0.8\n",
    "amount = int(percent_for_train*len(data_set))\n",
    "\n",
    "train_dataset = data_set[0:amount,:]\n",
    "test_dataset = data_set[amount+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, no_classes):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(50, no_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# print(Forward_model)\n",
    "\n",
    "# # Instance of the Neural Network, loss, optimizer\n",
    "# clf = NN(1,4).to('cuda')\n",
    "# opt = Adam(clf.parameters(),lr=1e-3)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "input_size = 160\n",
    "no_classes = 4\n",
    "learning_rate = 0.09\n",
    "batch_size = amount\n",
    "no_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "train_dataset =torch.tensor(train_dataset)\n",
    "test_dataset = torch.tensor(test_dataset)\n",
    "train_dataset = train_dataset.clone().detach()\n",
    "test_dataset = test_dataset.clone().detach()\n",
    "\n",
    "\n",
    "trt = datasets.MNIST(root='dataset/',train=True,transform=transforms.ToTensor(),download=True)\n",
    "tet = datasets.MNIST(root='dataset/',train=False,transform=transforms.ToTensor(),download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=trt,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(dataset=tet,batch_size=40,shuffle=True)\n",
    "\n",
    "model = NN(input_size=input_size,no_classes=no_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cross = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Network\n",
    "# for epoch in range(no_epochs):\n",
    "#     for batch_idx, (data, targets) in enumerate(train_dataset):\n",
    "#         data = data.to(device=device)\n",
    "#         targets = targets.to(device=device)\n",
    "\n",
    "#         # Forward\n",
    "#         scores = model(data)\n",
    "#         loss = cross\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "L = 200\n",
    "sigma = 0.2\n",
    "eta = 0.09\n",
    "\n",
    "input = np.array([[-3],[-1],[1],[3]])\n",
    "output = np.array([[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]])\n",
    "#output = np.array([[-3],[-1],[1],[3]])\n",
    "\n",
    "\n",
    "data_set_t = []\n",
    "data_set_x = []\n",
    "for i in range(L):\n",
    "    n = numpy.random.randint(0,4)\n",
    "    data_set_t.append(output[n])\n",
    "    data_set_x.append(input[n])\n",
    "\n",
    "#data_set_t_noise = data_set_t + sigma*np.random.randn(len(data_set_t),4)\n",
    "\n",
    "data_set = np.hstack((data_set_x,data_set_t))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return expit(-x) * (1-expit(-x))\n",
    "\n",
    "percent_for_train = 0.8\n",
    "amount = int(percent_for_train*len(data_set))\n",
    "\n",
    "train_dataset = data_set[0:amount,:]\n",
    "test_dataset = data_set[amount+1:,:]\n",
    "\n",
    "train_dataset =torch.tensor(train_dataset)\n",
    "test_dataset = torch.tensor(test_dataset)\n",
    "train_dataset = train_dataset.clone().detach()\n",
    "test_dataset = test_dataset.clone().detach()\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 50)\n",
    "        self.fc2 = nn.Linear(50, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the training data\n",
    "input_data = torch.randn(100, 1)\n",
    "\n",
    "input_data = train_dataset[:,0]\n",
    "input_data = torch.reshape(input_data,(amount,1))\n",
    "input_data = input_data.to(torch.float32)\n",
    "\n",
    "\n",
    "# Define the labels\n",
    "#labels = torch.tensor([[-3, -1, 1, 3]])\n",
    "\n",
    "labels = train_dataset[:,1:]\n",
    "labels = torch.reshape(labels,(amount,4))\n",
    "labels = labels.to(torch.float32)\n",
    "\n",
    "# Define the model\n",
    "net = Net()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.09)\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "for epoch in range(1000):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(input_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass and update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the output on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_dataset[:,0]\n",
    "test_input = torch.reshape(test_input,(39,1))\n",
    "test_input = test_input.to(torch.float32)\n",
    "output = net(test_input)\n",
    "t_calc = []\n",
    "for i in range(len(output)):\n",
    "    result = [tensor.item() for tensor in output[i]]\n",
    "    t_calc.append(result)\n",
    "\n",
    "t = []\n",
    "for i in range(len(test_dataset)):\n",
    "    result = [tensor.item() for tensor in test_dataset[i,1:]]\n",
    "    t.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classification errors with logistic regression is: 0 out of 39 \n",
      "which is 0.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classification_errors = 0\n",
    "for i in range(len(output)):\n",
    "    if np.argmax(t_calc[i]) == np.argmax(t[i]):\n",
    "        # Succes!\n",
    "        continue\n",
    "    else:\n",
    "        classification_errors = classification_errors + 1    \n",
    "\n",
    "print(f'The number of classification errors with logistic regression is: {classification_errors} out of {len(t)} \\nwhich is {classification_errors/len(t)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
